# SR-IOV (Single Root I/O Virtualization) Support

This document describes SR-IOV support in Vibe-VMM for high-performance device passthrough.

## Overview

**SR-IOV** allows a single physical device to present multiple **Virtual Functions (VFs)** that can be directly assigned to guest VMs, providing near-native performance by bypassing VMM device emulation.

### Use Cases

- **High-performance networking** - 10Gbps+ line rate without CPU overhead
- **Low-latency storage** - NVMe passthrough for databases
- **GPU virtualization** - SR-IOV vGPU for graphics/CPU workloads
- **FPGA acceleration** - Direct FPGA access from guest

## Architecture

```
Physical Function (PF)
â”œâ”€â”€ VF 0 â†’ Direct assigned to VM 1
â”œâ”€â”€ VF 1 â†’ Direct assigned to VM 2
â”œâ”€â”€ VF 2 â†’ Direct assigned to VM 3
â””â”€â”€ VF 3 â†’ Direct assigned to VM 4
```

Each VF appears as a separate PCI device to the guest, with direct hardware access through the IOMMU.

## Current Implementation Status

### âœ… Already Implemented (Linux)

- VFIO container/group/device management (`src/vfio.c`)
- PCI config space emulation
- MMIO region mapping to guest memory
- IRQ setup infrastructure
- CLI argument: `--vfio <BDF>`

### ðŸš§ Limitations

- **Linux only** - VFIO is Linux-specific
- **No macOS support** - macOS doesn't have VFIO
- **Basic PCI emulation** - No hotplug, no reset
- **INTx only** - MSI/MSI-X not fully implemented
- **No documentation** - Usage guides needed

## Implementation Guide

### 1. Host Preparation (Before Running VMM)

#### Create Virtual Functions

```bash
# Check if device supports SR-IOV
cat /sys/bus/pci/devices/0000:01:00.0/sriov_totalvfs

# Enable VFs (creates 4 VFs)
echo 4 > /sys/bus/pci/devices/0000:01:00.0/sriov_numvfs

# Verify VFs were created
ls -l /sys/bus/pci/devices/0000:01:00.0/virtfn*

# Example output:
# lrwxrwxrwx 1 root root 0 ... virtfn0 -> ../../../0000:01:00.1
# lrwxrwxrwx 1 root root 0 ... virtfn1 -> ../../../0000:01:00.2
# lrwxrwxrwx 1 root root 0 ... virtfn2 -> ../../../0000:01:00.3
# lrwxrwxrwx 1 root root 0 ... virtfn3 -> ../../../0000:01:00.4
```

#### Unbind VF from Host Driver

```bash
# Unbind VF from host driver (if bound)
virsh nodedev-detach pci_0000_01_00_1

# Or manually:
echo 0000:01:00.1 > /sys/bus/pci/drivers/ixgbe/unbind
```

#### Verify IOMMU Enabled

```bash
# Check IOMMU is enabled
dmesg | grep -e DMAR -e IOMMU

# Should show something like:
# DMAR: IOMMU enabled
```

### 2. VMM Usage

#### Assign VF to VM

```bash
# Basic usage
./bin/vibevmm \
  --kernel /path/to/vmlinuz \
  --initrd /path/to/initrd \
  --mem 2G \
  --vfio 0000:01:00.1

# With multiple VFs (future enhancement)
./bin/vibevmm \
  --kernel vmlinuz \
  --mem 2G \
  --vfio 0000:01:00.1 \
  --vfio 0000:01:00.2
```

#### Guest Configuration

Inside the guest, load the appropriate driver:

```bash
# Inside guest VM
modprobe ixgvf  # Intel VF driver
# OR
modprobe igbvf  # Intel I350 VF driver
# OR
modprobe nvme   # For NVMe passthrough
```

## VFIO Implementation Details

### Device Initialization Flow

```c
// 1. Create VFIO container
struct vfio_container *container = vfio_container_create();

// 2. Open VFIO device (e.g., 0000:01:00.1)
struct vfio_dev *vdev = vfio_device_open(container, "0000:01:00.1");

// 3. Get device info
struct vfio_device_info info;
vfio_device_get_info(vdev, &info);
printf("Device has %d regions, %d IRQs\n", info.num_regions, info.num_irqs);

// 4. Map BARs to guest memory
for (int i = 0; i < info.num_regions; i++) {
    uint64_t gpa = allocate_guest_bar_range(region_size);
    vfio_map_region(vdev, i, gpa);
}

// 5. Setup IRQs
vfio_setup_irqs(vdev, VFIO_PCI_INTX_IRQ_INDEX, VFIO_IRQ_INFO_EVENTFD);

// 6. Register with VM
vfio_register_with_vm(vm, vdev, "0000:01:00.1");
```

### Memory Layout

```
Guest Physical Address Space:

0x00000000 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚     Guest RAM         â”‚
â”‚     (2GB)             â”‚
0x7FFFFFFF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚                       â”‚
0xE0000000 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚  VFIO BAR0 (MMIO)     â”‚  â† VF device registers
0xEFFFFFFF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚                       â”‚
0xF0000000 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚  VFIO BAR1 (MMIO)     â”‚  â† VF device memory
0xF0000FFF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚                       â”‚
0xF0001000 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚  PCI Config Space     â”‚  â† Emulated PCI config
â”‚  (4KB)                â”‚
0xF0001FFF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### PCI Configuration Space

The VF device appears as a PCI device to the guest:

```
00:01.0 Ethernet controller: Intel Corporation X710 Virtual Function
    Vendor ID: 0x8086
    Device ID: 0x154c
    Class: Ethernet controller
    BAR0: Memory at 0xe0000000 (32-bit, non-prefetchable) [size=64K]
    BAR1: Memory at 0xe0010000 (32-bit, non-prefetchable) [size=64K]
    IRQ: 20
```

## Platform Support

### Linux (x86_64) âœ… Full Support

- VFIO framework available
- KVM + VFIO integration
- IOMMU (Intel VT-d or AMD-Vi)

### macOS âŒ Not Supported

- macOS doesn't have VFIO
- Hypervisor.framework doesn't support device assignment
- Alternative: Virtio devices for network/block

## Performance Comparison

| Method | Throughput | CPU Usage | Latency |
|--------|-----------|-----------|---------|
| Virtio-net | 5-8 Gbps | 20-30% | Medium |
| VFIO Passthrough | 10 Gbps+ | 2-5% | Low |
| SR-IOV VF | 10 Gbps+ | 2-5% | Low |

SR-IOV provides **near-native performance** because:
- Direct hardware access (no emulation)
- DMA bypasses VMM (via IOMMU)
- Interrupts delivered directly to guest

## Security Considerations

### IOMMU Isolation

The IOMMU provides security by:
1. **DMA isolation** - Each VF can only access its assigned memory
2. **Address translation** - Guest physical â†’ Host physical translation
3. **Access control** - VMM controls what memory each VF can access

### Best Practices

```bash
# 1. Enable IOMMU in kernel
# GRUB: intel_iommu=on or amd_iommu=on

# 2. Use VFs with trusted guests only
# VFIO bypasses VMM for DMA

# 3. Monitor forVFIO-related errors
dmesg | grep -i vfio

# 4. Use cgroups to limit device access
# (for multi-tenant environments)
```

## Troubleshooting

### Common Issues

#### "Failed to create VFIO container"
```bash
# Check VFIO module is loaded
lsmod | grep vfio

# Load VFIO modules
modprobe vfio
modprobe vfio_pci
modprobe vfio_iommu_type1
```

#### "Device is in use by host driver"
```bash
# Unbind from host driver
echo 0000:01:00.1 > /sys/bus/pci/drivers/ixgbe/unbind

# Or use virsh
virsh nodedev-detach pci_0000_01_00_1
```

#### "IOMMU not enabled"
```bash
# Check kernel cmdline
cat /proc/cmdline | grep iommu

# Add to GRUB if missing:
# intel_iommu=on amd_iommu=on
```

### Debug Logging

```bash
# Enable VMM debug logging
./vibevmm --vfio 0000:01:00.1 --log 4 --mem 2G

# Check kernel messages
dmesg | grep -i vfio
dmesg | grep -i iommu
```

## Future Enhancements

### Short Term

- [ ] **Multiple VF support** - Assign multiple VFs to one VM
- [ ] **MSI/MSI-X support** - Better interrupt handling
- [ ] **Hotplug support** - Add/remove VFs at runtime
- [ ] **Device reset** - Proper VF reset on VM shutdown

### Long Term

- [ ] **macOS support** - Investigate alternatives (Hypervisor.framework extensions?)
- [ ] **ARM64 support** - VFIO on ARM64 with SMMU
- [ ] **Live migration** - Migration with VFIO devices
- [ ] **SR-IOV documentation** - User guides for specific devices

## Hardware Compatibility

### Tested NICs

| Device | PF Driver | VF Driver | SR-IOV VFs |
|--------|-----------|-----------|------------|
| Intel X710 | i40e | i40evf | 64 |
| Intel XL710 | i40e | i40evf | 64 |
| Intel X550 | ixgbe | ixgbevf | 8 |
| Intel I350 | igb | igbvf | 8 |
| Mellanox CX5 | mlx5_core | mlx5_core | 16 |

### Other Devices

- **NVMe** - Some support SR-IOV
- **GPU** - NVIDIA vGPU, AMD SR-IOV (limited)
- **FPGA** - Intel FPGA, Xilinx (device-specific)

## References

- [VFIO Documentation](https://www.kernel.org/doc/html/latest/driver-api/vfio.html)
- [SR-IOV Wikipedia](https://en.wikipedia.org/wiki/Single-root_input/output_virtualization)
- [Intel X710 SR-IOV Guide](https://www.intel.com/content/www/us/en/programmable/documentation/)

## Example: Complete Setup

### Host Setup

```bash
#!/bin/bash
# setup_sriov_host.sh

# 1. Create VFs
echo "Creating 4 VFs on 0000:01:00.0..."
echo 4 > /sys/bus/pci/devices/0000:01:00.0/sriov_numvfs

# 2. Unbind VFs from host
for vf in /sys/bus/pci/devices/0000:01:00.0/virtfn*; do
    bdf=$(basename $(readlink $vf))
    echo "Unbinding $bdf from host..."
    echo $bdf > /sys/bus/pci/drivers/ixgbe/unbind
done

# 3. Load VFIO modules
modprobe vfio_pci
modprobe vfio_iommu_type1

# 4. List available VFs
echo "Available VFs:"
ls -l /sys/bus/pci/devices/0000:01:00.0/virtfn*
```

### Guest Launch

```bash
#!/bin/bash
# launch_vm_with_vf.sh

VF_BDF="0000:01:00.1"

./bin/vibevmm \
  --kernel /boot/vmlinuz-$(uname -r) \
  --initrd /boot/initrd.img-$(uname -r) \
  --mem 2G \
  --cpus 2 \
  --vfio $VF_BDF \
  --console
```

### Guest Verification

```bash
# Inside guest VM

# 1. Check for PCI device
lspci | grep Ethernet
# 01:00.0 Ethernet controller: Intel Corporation X710 Virtual Function

# 2. Load driver
modprobe i40evf

# 3. Check interface
ip link show
# 3: eth0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT

# 4. Performance test
iperf3 -s &  # Server
iperf3 -c <server_ip> -t 60  # Client (should show >9 Gbps)
```

---

**Document Version:** 1.0
**Last Updated:** 2026-02-06
**Platform:** Linux x86_64 with VFIO
